{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Live Camera Detection (ONNX) - Glute Bridge\n",
    "\n",
    "**Objective:**\n",
    "1.  Load the self-contained ONNX model.\n",
    "2.  Initialize MediaPipe Pose for landmark detection.\n",
    "3.  Capture video from a webcam.\n",
    "4.  In a real-time loop, for each frame:\n",
    "    a.  Extract pose landmarks.\n",
    "    b.  Calculate geometric features (angles, distances).\n",
    "    c.  Feed the features directly to the ONNX model for inference (scaling is handled internally).\n",
    "    d.  Implement logic for state tracking, rep counting, and user feedback.\n",
    "    e.  Display all information on the video frame.\n",
    "\n",
    "**Note:** This notebook runs an OpenCV window. To stop the camera feed, press **'q'** while the window is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add root project directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.geometry_utils import GeometryUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model and Paths ---\n",
    "MODEL_ONNX_DIR = \"../models/onnx/\"\n",
    "# >>>>> YOU CAN CHANGE THIS VALUE TO 'LR', 'KNN', 'DT', 'RF', or 'XGB' if you saved it earlier <<<<<\n",
    "MODEL_TYPE_TO_LOAD = \"RF\"\n",
    "ONNX_MODEL_PATH = os.path.join(MODEL_ONNX_DIR, f\"{MODEL_TYPE_TO_LOAD}_model.onnx\")\n",
    "\n",
    "# --- Landmark & Feature Definitions ---\n",
    "LANDMARK_NAMES = [\n",
    "    'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer', 'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "    'left_ear', 'right_ear', 'mouth_left', 'mouth_right', 'left_shoulder', 'right_shoulder', 'left_elbow',\n",
    "    'right_elbow', 'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky', 'left_index', 'right_index',\n",
    "    'left_thumb', 'right_thumb', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle',\n",
    "    'right_ankle', 'left_heel', 'right_heel', 'left_foot_index', 'right_foot_index'\n",
    "]\n",
    "\n",
    "FEATURE_COLUMN_NAMES = [\n",
    "    'left_elbow_angle', 'right_elbow_angle', 'left_shoulder_angle', 'right_shoulder_angle', \n",
    "    'left_hip_angle', 'right_hip_angle', 'left_knee_angle', 'right_knee_angle', \n",
    "    'left_body_align_angle', 'right_body_align_angle', 'left_hip_deviation', 'right_hip_deviation', \n",
    "    'shoulder_hip_y_diff_left', 'shoulder_hip_y_diff_right', 'hip_ankle_y_diff_left', 'hip_ankle_y_diff_right',\n",
    "    'torso_length_left', 'torso_length_right', 'leg_length_left', 'leg_length_right'\n",
    "]\n",
    "\n",
    "# --- Logic & Thresholds ---\n",
    "VISIBILITY_THRESHOLD = 0.6\n",
    "PREDICTION_SMOOTHING_WINDOW = 5\n",
    "CONFIDENCE_THRESHOLD_UP = 0.70\n",
    "CONFIDENCE_THRESHOLD_DOWN = 0.70\n",
    "HOLD_TIME_GOAL = 1.5  # seconds to hold the 'up' pose\n",
    "MIN_HOLD_FOR_REP = 0.5 # minimum time in 'up' pose to count as a rep\n",
    "\n",
    "# --- Display Configuration ---\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "COLOR_CORRECT = (0, 255, 0)\n",
    "COLOR_INCORRECT = (0, 0, 255)\n",
    "COLOR_WARNING = (0, 165, 255)\n",
    "COLOR_NEUTRAL = (255, 255, 255)\n",
    "COLOR_PROGRESS_BAR = (0, 255, 0)\n",
    "COLOR_PROGRESS_BG = (100, 100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load ONNX Model and Initialize Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model loaded successfully.\n",
      "  Input Name: float_input\n",
      "  Output Names: ['output_label', 'output_probability']\n"
     ]
    }
   ],
   "source": [
    "onnx_session = None\n",
    "input_name = None\n",
    "output_names = None\n",
    "\n",
    "if not os.path.exists(ONNX_MODEL_PATH):\n",
    "    print(f\"FATAL: ONNX model not found at {ONNX_MODEL_PATH}\")\n",
    "    print(\"Please run Notebook 4 to convert the PKL model to ONNX.\")\n",
    "else:\n",
    "    try:\n",
    "        onnx_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
    "        input_name = onnx_session.get_inputs()[0].name\n",
    "        output_names = [output.name for output in onnx_session.get_outputs()]\n",
    "        print(\"ONNX Model loaded successfully.\")\n",
    "        print(f\"  Input Name: {input_name}\")\n",
    "        print(f\"  Output Names: {output_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ONNX model: {e}\")\n",
    "        onnx_session = None\n",
    "\n",
    "# Initialize MediaPipe and Geometry Utilities\n",
    "mp_pose = mp.solutions.pose\n",
    "pose_detector = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "geo_utils = GeometryUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Helper Functions for Live Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_live_landmarks(landmarks):\n",
    "    \"\"\"Extracts features from live MediaPipe landmarks.\"\"\"\n",
    "    lm_coords = {name: [np.nan, np.nan] for name in LANDMARK_NAMES}\n",
    "    for i, lm in enumerate(landmarks.landmark):\n",
    "        if i < len(LANDMARK_NAMES) and lm.visibility > VISIBILITY_THRESHOLD:\n",
    "            lm_coords[LANDMARK_NAMES[i]] = [lm.x, lm.y]\n",
    "\n",
    "    # This inner function is just for cleaner code below\n",
    "    def get_coord(name): return lm_coords.get(name, [np.nan, np.nan])\n",
    "    \n",
    "    # Identical feature extraction logic as in training\n",
    "    features = {}\n",
    "    features['left_elbow_angle'] = geo_utils.calculate_angle(get_coord('left_shoulder'), get_coord('left_elbow'), get_coord('left_wrist'))\n",
    "    features['right_elbow_angle'] = geo_utils.calculate_angle(get_coord('right_shoulder'), get_coord('right_elbow'), get_coord('right_wrist'))\n",
    "    features['left_shoulder_angle'] = geo_utils.calculate_angle(get_coord('left_elbow'), get_coord('left_shoulder'), get_coord('left_hip'))\n",
    "    features['right_shoulder_angle'] = geo_utils.calculate_angle(get_coord('right_elbow'), get_coord('right_shoulder'), get_coord('right_hip'))\n",
    "    features['left_hip_angle'] = geo_utils.calculate_angle(get_coord('left_shoulder'), get_coord('left_hip'), get_coord('left_knee'))\n",
    "    features['right_hip_angle'] = geo_utils.calculate_angle(get_coord('right_shoulder'), get_coord('right_hip'), get_coord('right_knee'))\n",
    "    features['left_knee_angle'] = geo_utils.calculate_angle(get_coord('left_hip'), get_coord('left_knee'), get_coord('left_ankle'))\n",
    "    features['right_knee_angle'] = geo_utils.calculate_angle(get_coord('right_hip'), get_coord('right_knee'), get_coord('right_ankle'))\n",
    "    features['left_body_align_angle'] = geo_utils.calculate_angle(get_coord('left_shoulder'), get_coord('left_hip'), get_coord('left_ankle'))\n",
    "    features['right_body_align_angle'] = geo_utils.calculate_angle(get_coord('right_shoulder'), get_coord('right_hip'), get_coord('right_ankle'))\n",
    "    features['left_hip_deviation'] = geo_utils.distance_point_to_line(get_coord('left_hip'), get_coord('left_shoulder'), get_coord('left_knee'))\n",
    "    features['right_hip_deviation'] = geo_utils.distance_point_to_line(get_coord('right_hip'), get_coord('right_shoulder'), get_coord('right_knee'))\n",
    "    ls_y, lh_y = get_coord('left_shoulder')[1], get_coord('left_hip')[1]\n",
    "    features['shoulder_hip_y_diff_left'] = abs(ls_y - lh_y) if not (np.isnan(ls_y) or np.isnan(lh_y)) else np.nan\n",
    "    rs_y, rh_y = get_coord('right_shoulder')[1], get_coord('right_hip')[1]\n",
    "    features['shoulder_hip_y_diff_right'] = abs(rs_y - rh_y) if not (np.isnan(rs_y) or np.isnan(rh_y)) else np.nan\n",
    "    la_y = get_coord('left_ankle')[1]\n",
    "    features['hip_ankle_y_diff_left'] = abs(lh_y - la_y) if not (np.isnan(lh_y) or np.isnan(la_y)) else np.nan\n",
    "    ra_y = get_coord('right_ankle')[1]\n",
    "    features['hip_ankle_y_diff_right'] = abs(rh_y - ra_y) if not (np.isnan(rh_y) or np.isnan(ra_y)) else np.nan\n",
    "    features['torso_length_left'] = geo_utils.calculate_distance(get_coord('left_shoulder'), get_coord('left_hip'))\n",
    "    features['torso_length_right'] = geo_utils.calculate_distance(get_coord('right_shoulder'), get_coord('right_hip'))\n",
    "    features['leg_length_left'] = geo_utils.calculate_distance(get_coord('left_hip'), get_coord('left_ankle'))\n",
    "    features['leg_length_right'] = geo_utils.calculate_distance(get_coord('right_hip'), get_coord('right_ankle'))\n",
    "    \n",
    "    # Return as a numpy array in the correct order\n",
    "    return np.array([features[name] for name in FEATURE_COLUMN_NAMES], dtype=np.float32).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Main Video Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_glute_bridge_detector():\n",
    "    if onnx_session is None:\n",
    "        print(\"Cannot run detector because the ONNX model is not loaded.\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(0) # Use 0 for webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open webcam.\")\n",
    "        return\n",
    "\n",
    "    # State variables\n",
    "    pose_state = 'down'\n",
    "    rep_counter = 0\n",
    "    feedback = \"Start in the down position\"\n",
    "    hold_start_time = None\n",
    "    hold_duration = 0.0\n",
    "\n",
    "    # For smoothing predictions\n",
    "    proba_history = deque(maxlen=PREDICTION_SMOOTHING_WINDOW)\n",
    "    \n",
    "    print(\"Starting Glute Bridge Detector... Press 'q' to quit.\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1) # Flip for selfie view\n",
    "        h, w, _ = frame.shape\n",
    "\n",
    "        # Pose detection\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose_detector.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            # Extract features\n",
    "            live_features = extract_features_from_live_landmarks(results.pose_landmarks)\n",
    "            \n",
    "            # Check if features are valid before prediction\n",
    "            if np.isnan(live_features).sum() > len(FEATURE_COLUMN_NAMES) * 0.5:\n",
    "                feedback = \"Body not fully visible\"\n",
    "                proba_history.clear()\n",
    "            else:\n",
    "                \n",
    "                # Inference\n",
    "                onnx_outputs = onnx_session.run(output_names, {input_name: live_features})\n",
    "                # onnx_outputs[1] is the probability output.\n",
    "                prob_output = onnx_outputs[1] \n",
    "                \n",
    "                # The output can be a list of dicts [{0: prob, 1: prob}] or an array [[prob, prob]].\n",
    "                # We need to parse it into a simple numpy array.\n",
    "                if isinstance(prob_output, list) and len(prob_output) > 0 and isinstance(prob_output[0], dict):\n",
    "                    prob_dict = prob_output[0]\n",
    "                    # Create array ordered by class index (0, 1)\n",
    "                    probabilities = np.array([prob_dict.get(0, 0.0), prob_dict.get(1, 0.0)], dtype=np.float32)\n",
    "                elif isinstance(prob_output, np.ndarray):\n",
    "                    # Handles the case where output is already like [[0.1, 0.9]]\n",
    "                    probabilities = prob_output[0]\n",
    "                else:\n",
    "                    # Fallback for unexpected formats\n",
    "                    print(f\"Warning: Unexpected ONNX probability format: {type(prob_output)}\")\n",
    "                    probabilities = np.array([0.5, 0.5]) # Default to uncertainty\n",
    "\n",
    "                # Now, 'probabilities' is guaranteed to be a numpy array\n",
    "                proba_history.append(probabilities)\n",
    "\n",
    "                if len(proba_history) == PREDICTION_SMOOTHING_WINDOW:\n",
    "                    smoothed_proba = np.mean(proba_history, axis=0) # This will now work\n",
    "                    pred_label = np.argmax(smoothed_proba)\n",
    "                    confidence = smoothed_proba[pred_label]\n",
    "\n",
    "                    # State machine for rep counting\n",
    "                    if pose_state == 'down':\n",
    "                        if pred_label == 1 and confidence > CONFIDENCE_THRESHOLD_UP: # Transition to UP\n",
    "                            pose_state = 'up'\n",
    "                            hold_start_time = time.time()\n",
    "                            feedback = \"Hold it...\"\n",
    "                        else:\n",
    "                            feedback = \"Lift your hips\"\n",
    "                            hold_duration = 0.0\n",
    "                            hold_start_time = None\n",
    "                            \n",
    "                    elif pose_state == 'up':\n",
    "                        if pred_label == 0 and confidence > CONFIDENCE_THRESHOLD_DOWN: # Transition to DOWN\n",
    "                            if hold_duration >= MIN_HOLD_FOR_REP:\n",
    "                                rep_counter += 1\n",
    "                                feedback = f\"Rep {rep_counter} counted!\"\n",
    "                            else:\n",
    "                                feedback = \"Hold longer next time!\"\n",
    "                            pose_state = 'down'\n",
    "                            hold_duration = 0.0\n",
    "                            hold_start_time = None\n",
    "                        else: # Still in UP state\n",
    "                            hold_duration = time.time() - hold_start_time\n",
    "                            if hold_duration >= HOLD_TIME_GOAL:\n",
    "                                feedback = \"Great hold! Lower slowly.\"\n",
    "                            else:\n",
    "                                feedback = f\"Holding... {hold_duration:.1f}s\"\n",
    "        else:\n",
    "            feedback = \"No person detected\"\n",
    "            proba_history.clear()\n",
    "\n",
    "        # --- Display UI --- \n",
    "        # Rep Counter Box\n",
    "        cv2.rectangle(frame, (0, 0), (250, 80), (20, 20, 20), -1)\n",
    "        cv2.putText(frame, 'REPS', (15, 30), FONT, 0.8, COLOR_NEUTRAL, 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, str(rep_counter), (100, 70), FONT, 2, COLOR_NEUTRAL, 3, cv2.LINE_AA)\n",
    "\n",
    "        # Feedback Box\n",
    "        cv2.rectangle(frame, (250, 0), (w, 80), (20, 20, 20), -1)\n",
    "        cv2.putText(frame, 'FEEDBACK', (265, 30), FONT, 0.8, COLOR_NEUTRAL, 2, cv2.LINE_AA)\n",
    "        feedback_color = COLOR_CORRECT if 'Great' in feedback or 'counted' in feedback else COLOR_WARNING\n",
    "        cv2.putText(frame, feedback, (265, 65), FONT, 1, feedback_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        # Hold Progress Bar\n",
    "        if pose_state == 'up':\n",
    "            progress = min(1.0, hold_duration / HOLD_TIME_GOAL)\n",
    "            bar_width = int(progress * (w - 20))\n",
    "            cv2.rectangle(frame, (10, h - 30), (w - 10, h - 10), COLOR_PROGRESS_BG, -1)\n",
    "            cv2.rectangle(frame, (10, h - 30), (10 + bar_width, h - 10), COLOR_PROGRESS_BAR, -1)\n",
    "\n",
    "        # Display Frame\n",
    "        cv2.imshow('Glute Bridge Detector', frame)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # This is important for Jupyter to properly release the window\n",
    "    for i in range(5):\n",
    "        cv2.waitKey(1)\n",
    "    print(\"Detector stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Glute Bridge Detector... Press 'q' to quit.\n",
      "Detector stopped.\n"
     ]
    }
   ],
   "source": [
    "# Run the main loop\n",
    "run_glute_bridge_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
