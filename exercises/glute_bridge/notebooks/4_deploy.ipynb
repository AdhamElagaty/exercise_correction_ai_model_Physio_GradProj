{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Deployment (PKL to ONNX) - Glute Bridge\n",
    "\n",
    "**Objective:**\n",
    "1.  Load the saved `scikit-learn` pipeline from its `.pkl` file.\n",
    "2.  Convert the entire pipeline (which includes the imputer, scaler, and model) into a single ONNX file.\n",
    "3.  Save the ONNX model to the `models/onnx/` directory, making it ready for the live detection application.\n",
    "\n",
    "**Note:** Since we embedded the scaler within the pipeline, we **do not** need to create a separate `scaler.json` file. The ONNX model will handle both scaling and inference internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import onnxruntime as ort # To verify the converted model\n",
    "import pickle\n",
    "\n",
    "# Add root project directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.model_deploy_convertor_utils import ModelDeployConverterUtils\n",
    "\n",
    "# Configure logger for better feedback from the utility\n",
    "logger = logging.getLogger('utils.model_deploy_convertor_utils')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PKL_DIR = \"../models/pkl/\"\n",
    "MODEL_ONNX_DIR = \"../models/onnx/\"\n",
    "os.makedirs(MODEL_ONNX_DIR, exist_ok=True)\n",
    "\n",
    "# <<< CHOOSE THE MODEL TO CONVERT >>>\n",
    "# This should be the base name of the .pkl file (without extension)\n",
    "# >>>>> YOU CAN CHANGE THIS VALUE TO 'LR', 'KNN', 'DT', 'RF', or 'XGB' if you saved it earlier <<<<<\n",
    "MODEL_TYPE_TO_LOAD = \"RF\" \n",
    "\n",
    "TARGET_OPSET = 12 # A common and widely supported ONNX opset version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Perform Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 21:25:41,468 - utils.model_deploy_convertor_utils - INFO - Starting conversion for model 'RF_model': ../models/pkl/RF_model.pkl -> ../models/onnx/RF_model.onnx\n",
      "2025-06-07 21:25:41,473 - utils.model_deploy_convertor_utils - INFO - Successfully loaded pickle model from ../models/pkl/RF_model.pkl\n",
      "2025-06-07 21:25:41,474 - utils.model_deploy_convertor_utils - INFO - Model for 'RF_model' expects 20 input features.\n",
      "2025-06-07 21:25:41,475 - utils.model_deploy_convertor_utils - INFO - Attempting to convert model 'RF_model' to ONNX with target_opset=12.\n",
      "2025-06-07 21:25:41,519 - utils.model_deploy_convertor_utils - INFO - Successfully converted model 'RF_model' to ONNX format.\n",
      "2025-06-07 21:25:41,521 - utils.model_deploy_convertor_utils - INFO - Successfully saved ONNX model to ../models/onnx/RF_model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Conversion for RF_model ---\n",
      "\n",
      "SUCCESS: Pipeline converted and saved to ONNX format at:\n",
      "==> ../models/onnx/RF_model.onnx\n",
      "\n",
      "--- Verifying the ONNX Model ---\n",
      "ONNX model loaded successfully.\n",
      "Input Name: float_input\n",
      "Output Names: ['output_label', 'output_probability']\n",
      "Expected number of input features: 20\n",
      "ERROR during ONNX model verification: name 'np' is not defined\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Starting Conversion for {MODEL_TYPE_TO_LOAD}_model ---\")\n",
    "\n",
    "source_pkl_path = os.path.join(MODEL_PKL_DIR, f\"{MODEL_TYPE_TO_LOAD}_model.pkl\")\n",
    "if not os.path.exists(source_pkl_path):\n",
    "    print(f\"ERROR: Source PKL file not found at: {source_pkl_path}\")\n",
    "    print(\"Please ensure you have run Notebook 2 to train and save the model.\")\n",
    "else:\n",
    "    # Use the ModelDeployConverterUtils to perform the conversion\n",
    "    onnx_path = ModelDeployConverterUtils.convert_model_pickle_to_onnx(\n",
    "        pickle_model_dir=MODEL_PKL_DIR, \n",
    "        onnx_model_dir=MODEL_ONNX_DIR, \n",
    "        name=f\"{MODEL_TYPE_TO_LOAD}_model\",\n",
    "        target_opset=TARGET_OPSET\n",
    "    )\n",
    "\n",
    "    if onnx_path and os.path.exists(onnx_path):\n",
    "        print(f\"\\nSUCCESS: Pipeline converted and saved to ONNX format at:\")\n",
    "        print(f\"==> {onnx_path}\")\n",
    "        \n",
    "        # --- Verification Step ---\n",
    "        print(\"\\n--- Verifying the ONNX Model ---\")\n",
    "        try:\n",
    "            # Load the PKL model to get the expected number of features\n",
    "            with open(source_pkl_path, 'rb') as f:\n",
    "                skl_pipeline = pickle.load(f)\n",
    "            \n",
    "            # Get the number of features from the imputer or scaler step\n",
    "            n_features = skl_pipeline.named_steps['imputer'].n_features_in_\n",
    "            \n",
    "            # Load the ONNX model\n",
    "            ort_session = ort.InferenceSession(onnx_path)\n",
    "            input_name = ort_session.get_inputs()[0].name\n",
    "            output_names = [output.name for output in ort_session.get_outputs()]\n",
    "            \n",
    "            print(f\"ONNX model loaded successfully.\")\n",
    "            print(f\"Input Name: {input_name}\")\n",
    "            print(f\"Output Names: {output_names}\")\n",
    "            print(f\"Expected number of input features: {n_features}\")\n",
    "            \n",
    "            # Test with a dummy input\n",
    "            dummy_input = np.random.rand(1, n_features).astype(np.float32)\n",
    "            onnx_outputs = ort_session.run(output_names, {input_name: dummy_input})\n",
    "            print(\"\\nDummy inference test successful!\")\n",
    "            print(f\"Predicted Label: {onnx_outputs[0]}\")\n",
    "            print(f\"Predicted Probabilities: {onnx_outputs[1]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during ONNX model verification: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nFAILURE: Model conversion to ONNX failed. Check the logs above for errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Summary\n",
    "\n",
    "The process is complete. The selected scikit-learn pipeline (`.pkl`) has been converted into a self-contained ONNX model.\n",
    "\n",
    "This single `.onnx` file now contains all the necessary logic for:\n",
    "1.  Imputing missing values.\n",
    "2.  Scaling the input features.\n",
    "3.  Making a classification prediction.\n",
    "\n",
    "This artifact is now ready to be used in the live camera detection notebook (`5_live_camera_detection.ipynb`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
