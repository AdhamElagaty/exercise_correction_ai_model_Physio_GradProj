{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Live Camera Detection (ONNX) - Plank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "1. Load the converted ONNX model pipeline.\n",
    "2. Initialize MediaPipe Pose for landmark detection.\n",
    "3. Capture video from a webcam or video file.\n",
    "4. For each frame, perform pose estimation, extract features, and use the ONNX model for inference.\n",
    "5. Implement feedback logic with smoothing and debouncing.\n",
    "6. Display the classification, confidence, and feedback on the video feed.\n",
    "\n",
    "**Note:** To stop the feed, press 'q' while the OpenCV window is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.geometry_utils import GeometryUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>> YOU CAN CHANGE THIS VALUE TO 'LR', 'KNN', 'DT', 'RF', or 'XGB' if you saved it earlier <<<<<\n",
    "MODEL_KEY = 'RF'\n",
    "ONNX_MODEL_PATH = f\"../models/onnx/{MODEL_KEY}_model.onnx\"\n",
    "VIDEO_SOURCE = 0\n",
    "\n",
    "# --- Detection & Logic Configuration ---\n",
    "VISIBILITY_THRESHOLD = 0.5\n",
    "SMOOTHING_WINDOW_SIZE = 7\n",
    "STATUS_HISTORY_LENGTH = 10\n",
    "STATUS_COOLDOWN_SEC = 0.5\n",
    "CONFIDENCE_THRESHOLD_CORRECT = 0.65\n",
    "CONFIDENCE_THRESHOLD_WRONG = 0.50\n",
    "\n",
    "# --- UI & Color Configuration ---\n",
    "COLORS = {\n",
    "    'landmark_main': (245, 117, 66),\n",
    "    'landmark_secondary': (245, 66, 230),\n",
    "    'status_correct': (0, 255, 0), # Green\n",
    "    'status_high_hips': (0, 0, 255), # Red\n",
    "    'status_low_hips': (0, 165, 255), # Orange\n",
    "    'status_analyzing': (255, 255, 0), # Cyan/Yellow\n",
    "    'text_primary': (255, 255, 255), # White\n",
    "    'text_feedback': (0, 255, 255)\n",
    "}\n",
    "LABEL_MAP = {0: \"CORRECT\", 1: \"HIGH HIPS\", 2: \"LOW HIPS\"}\n",
    "COLOR_MAP = {0: COLORS['status_correct'], 1: COLORS['status_high_hips'], 2: COLORS['status_low_hips']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Helper Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction logic (must be identical to training)\n",
    "LANDMARK_NAMES = [ 'nose', 'left_eye_inner', 'left_eye', 'left_eye_outer', 'right_eye_inner', 'right_eye', 'right_eye_outer', 'left_ear', 'right_ear', 'mouth_left', 'mouth_right', 'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_pinky', 'right_pinky', 'left_index', 'right_index', 'left_thumb', 'right_thumb', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle', 'left_heel', 'right_heel', 'left_foot_index', 'right_foot_index' ]\n",
    "geo_utils = GeometryUtils()\n",
    "\n",
    "def extract_features_from_landmarks(landmarks, visibility_threshold):\n",
    "    lm_coords = {}\n",
    "    for i, name in enumerate(LANDMARK_NAMES):\n",
    "        lm = landmarks.landmark[i]\n",
    "        if lm.visibility > visibility_threshold:\n",
    "            lm_coords[name] = [lm.x, lm.y]\n",
    "        else:\n",
    "            lm_coords[name] = [np.nan, np.nan]\n",
    "    \n",
    "    def get_coord(name): return lm_coords.get(name, [np.nan, np.nan])\n",
    "\n",
    "    features = []\n",
    "    # This order MUST match the training features exactly\n",
    "    features.append(geo_utils.calculate_angle(get_coord('left_shoulder'), get_coord('left_elbow'), get_coord('left_wrist')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('right_shoulder'), get_coord('right_elbow'), get_coord('right_wrist')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('left_elbow'), get_coord('left_shoulder'), get_coord('left_hip')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('right_elbow'), get_coord('right_shoulder'), get_coord('right_hip')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('left_shoulder'), get_coord('left_hip'), get_coord('left_knee')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('right_shoulder'), get_coord('right_hip'), get_coord('right_knee')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('left_hip'), get_coord('left_knee'), get_coord('left_ankle')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('right_hip'), get_coord('right_knee'), get_coord('right_ankle')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('left_shoulder'), get_coord('left_hip'), get_coord('left_ankle')))\n",
    "    features.append(geo_utils.calculate_angle(get_coord('right_shoulder'), get_coord('right_hip'), get_coord('right_ankle')))\n",
    "    features.append(geo_utils.distance_point_to_line(get_coord('left_hip'), get_coord('left_shoulder'), get_coord('left_knee')))\n",
    "    features.append(geo_utils.distance_point_to_line(get_coord('right_hip'), get_coord('right_shoulder'), get_coord('right_knee')))\n",
    "    features.append(abs(get_coord('left_shoulder')[1] - get_coord('left_hip')[1]))\n",
    "    features.append(abs(get_coord('right_shoulder')[1] - get_coord('right_hip')[1]))\n",
    "    features.append(abs(get_coord('left_hip')[1] - get_coord('left_ankle')[1]))\n",
    "    features.append(abs(get_coord('right_hip')[1] - get_coord('right_ankle')[1]))\n",
    "    features.append(geo_utils.calculate_distance(get_coord('left_shoulder'), get_coord('left_hip')))\n",
    "    features.append(geo_utils.calculate_distance(get_coord('right_shoulder'), get_coord('right_hip')))\n",
    "    features.append(geo_utils.calculate_distance(get_coord('left_hip'), get_coord('left_ankle')))\n",
    "    features.append(geo_utils.calculate_distance(get_coord('right_hip'), get_coord('right_ankle')))\n",
    "    return np.array([features], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Initialize Models and Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model '../models/onnx/RF_model.onnx' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# ONNX Runtime\n",
    "ort_session = None\n",
    "input_name = None\n",
    "try:\n",
    "    ort_session = ort.InferenceSession(ONNX_MODEL_PATH)\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    print(f\"ONNX model '{ONNX_MODEL_PATH}' loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ONNX model: {e}\")\n",
    "\n",
    "# OpenCV Video Capture\n",
    "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video source '{VIDEO_SOURCE}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Main Detection Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection loop finished.\n"
     ]
    }
   ],
   "source": [
    "def run_plank_detector():\n",
    "    if not cap.isOpened() or ort_session is None:\n",
    "        print(\"Setup failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # State variables\n",
    "    proba_history = deque(maxlen=SMOOTHING_WINDOW_SIZE)\n",
    "    status_log = deque(maxlen=STATUS_HISTORY_LENGTH)\n",
    "    last_status = \"ANALYZING\"\n",
    "    last_status_change_time = time.time()\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video stream.\")\n",
    "            break\n",
    "        \n",
    "        # Process frame\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        current_status = \"ANALYZING\"\n",
    "        status_color = COLORS['status_analyzing']\n",
    "        confidence_str = \"\"\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(image_bgr, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                      mp_drawing.DrawingSpec(color=COLORS['landmark_main'], thickness=2, circle_radius=2),\n",
    "                                      mp_drawing.DrawingSpec(color=COLORS['landmark_secondary'], thickness=2, circle_radius=2))\n",
    "            \n",
    "            # Feature Extraction\n",
    "            features = extract_features_from_landmarks(results.pose_landmarks, VISIBILITY_THRESHOLD)\n",
    "\n",
    "            if np.isnan(features).all():\n",
    "                current_status = \"POOR VISIBILITY\"\n",
    "            else:\n",
    "                # Inference with ONNX\n",
    "                ort_inputs = {input_name: features}\n",
    "                pred_label, pred_proba_list_of_dicts = ort_session.run(None, ort_inputs)\n",
    "\n",
    "                # FIX: Convert the ONNX probability output (list of dicts) to a simple numpy array\n",
    "                prob_dict = pred_proba_list_of_dicts[0]\n",
    "                prob_array = np.array([prob_dict[k] for k in sorted(prob_dict.keys())])\n",
    "                proba_history.append(prob_array)\n",
    "\n",
    "                # Smoothing and Debouncing\n",
    "                if len(proba_history) == SMOOTHING_WINDOW_SIZE:\n",
    "                    smoothed_proba = np.mean(proba_history, axis=0)\n",
    "                    final_prediction = np.argmax(smoothed_proba)\n",
    "                    confidence = smoothed_proba[final_prediction]\n",
    "                    confidence_str = f\"{confidence:.2f}\"\n",
    "                    \n",
    "                    status_log.append(final_prediction)\n",
    "                    \n",
    "                    # Debounce: Check for consistent status\n",
    "                    if len(status_log) == STATUS_HISTORY_LENGTH and len(set(status_log)) == 1:\n",
    "                        confirmed_status = status_log[0]\n",
    "                        if confirmed_status != last_status and (time.time() - last_status_change_time > STATUS_COOLDOWN_SEC):\n",
    "                            last_status = confirmed_status\n",
    "                            last_status_change_time = time.time()\n",
    "                    \n",
    "                    # Determine display status based on confidence\n",
    "                    if (last_status == 0 and confidence >= CONFIDENCE_THRESHOLD_CORRECT) or \\\n",
    "                       (last_status != 0 and confidence >= CONFIDENCE_THRESHOLD_WRONG):\n",
    "                        current_status = LABEL_MAP.get(last_status, \"UNKNOWN\")\n",
    "                        status_color = COLOR_MAP.get(last_status, COLORS['status_analyzing'])\n",
    "                    else:\n",
    "                        current_status = \"ADJUSTING\"\n",
    "                        \n",
    "        else:\n",
    "            current_status = \"NO POSE DETECTED\"\n",
    "            proba_history.clear()\n",
    "            status_log.clear()\n",
    "            last_status = \"ANALYZING\"\n",
    "\n",
    "        # UI Display\n",
    "        cv2.rectangle(image_bgr, (0,0), (image_bgr.shape[1], 80), (20, 20, 20), -1)\n",
    "        cv2.putText(image_bgr, 'STATUS', (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLORS['text_primary'], 2)\n",
    "        cv2.putText(image_bgr, current_status, (150, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2, cv2.LINE_AA)\n",
    "        cv2.putText(image_bgr, 'CONFIDENCE', (20, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, COLORS['text_primary'], 2)\n",
    "        cv2.putText(image_bgr, confidence_str, (180, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Plank Form Detector (ONNX)', image_bgr)\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # This is important for Jupyter to release the window properly\n",
    "    for i in range(5): cv2.waitKey(1)\n",
    "    pose.close()\n",
    "    print(\"Detection loop finished.\")\n",
    "\n",
    "# Run the main function\n",
    "run_plank_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
