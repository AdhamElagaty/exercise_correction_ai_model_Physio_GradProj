{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7aca236",
   "metadata": {},
   "source": [
    "# 5. Live Camera Detection (ONNX) - Bicep Curl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a045a0c",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "1. Load the ONNX models (left & right arm) and JSON scalers.\n",
    "2. Initialize MediaPipe Pose.\n",
    "3. Capture video from webcam.\n",
    "4. For each frame:\n",
    "    a. Perform pose estimation.\n",
    "    b. Extract arm landmarks.\n",
    "    c. Calculate angles using `GeometryUtils`.\n",
    "    d. Manually scale features using JSON scaler parameters.\n",
    "    e. Perform inference using the ONNX model to classify form.\n",
    "    f. Implement rep counting and feedback logic.\n",
    "    g. Display results on the frame.\n",
    "\n",
    "**Note:** This notebook will run an OpenCV window for live camera feed. Ensure you have a webcam connected. To stop the feed, press 'q' in the OpenCV window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50ff4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import onnxruntime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add utils directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('../../..')) \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utils.geometry_utils import GeometryUtils # For calculate_angle\n",
    "\n",
    "# IPython display for showing images in notebook (optional, mainly for static checks)\n",
    "from IPython.display import display, Image\n",
    "import threading # For running OpenCV loop in a way that doesn't block notebook kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab30bf6",
   "metadata": {},
   "source": [
    "## 5.1 Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bbdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "ONNX_MODEL_DIR = \"../models/onnx/\"\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# ==> example: \"NB\" for Normalized Body, \"RF\" for Random Forest, etc.\n",
    "MODEL_TYPE_LEFT = \"NB\"\n",
    "MODEL_TYPE_RIGHT = \"NB\"\n",
    "\n",
    "# --- MediaPipe Configuration ---\n",
    "MP_MIN_DETECTION_CONFIDENCE = 0.6\n",
    "MP_MIN_TRACKING_CONFIDENCE = 0.6\n",
    "MP_MODEL_COMPLEXITY = 1 # 0, 1, or 2. Higher is more accurate but slower.\n",
    "\n",
    "# --- Frame Configuration ---\n",
    "FRAME_WIDTH = 960 # If too large, might be slow\n",
    "FRAME_HEIGHT = 540\n",
    "\n",
    "# --- Arm Processing Logic Configuration ---\n",
    "MIN_VISIBILITY = 0.6 # Stricter than original script's 0.5 for potentially better quality angles\n",
    "SMOOTH_WINDOW_SIZE = 5 # For angle smoothing\n",
    "\n",
    "# Rep Counting Thresholds & Parameters (same as your provided detection script)\n",
    "ELBOW_ANGLE_UP_THRESHOLD = 85     # Angle to consider 'UP' state (e.g., elbow < 85 degrees)\n",
    "ELBOW_ANGLE_DOWN_THRESHOLD = 140  # Angle to consider 'DOWN' state (e.g., elbow > 140 degrees)\n",
    "SHOULDER_TOLERANCE = 25           # Max allowed deviation from neutral shoulder angle\n",
    "SHOULDER_CALIBRATION_ANGLE_MIN = 140 # Min elbow angle to start calibrating shoulder\n",
    "REP_COOLDOWN_SECONDS = 0.5        # Min time between reps\n",
    "MIN_REP_DURATION_SECONDS = 0.4    # Min duration for a valid rep\n",
    "MAX_REP_DURATION_SECONDS = 3.0    # Max duration for a valid rep\n",
    "ELBOW_EXTENSION_TARGET = 140      # For feedback: target elbow angle in down phase\n",
    "ELBOW_SQUEEZE_TARGET = 75         # For feedback: target elbow angle in up phase\n",
    "\n",
    "# --- Display Configuration ---\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "COLOR_CORRECT_FORM = (0, 255, 0)      # Green\n",
    "COLOR_INCORRECT_FORM = (0, 0, 255)    # Red\n",
    "COLOR_NEUTRAL = (255, 255, 255)     # White\n",
    "COLOR_FEEDBACK = (255, 200, 0)      # Light Blue/Orange for feedback text\n",
    "COLOR_VIS_LOW = (0, 165, 255)       # Orange for low visibility\n",
    "COLOR_MODEL_ERROR = (128, 0, 128)   # Purple for model/scaler load errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f0cf9",
   "metadata": {},
   "source": [
    "## 5.2 Initialize MediaPipe and GeometryUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcf4366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose_detector = mp_pose.Pose(\n",
    "    min_detection_confidence=MP_MIN_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MP_MIN_TRACKING_CONFIDENCE,\n",
    "    model_complexity=MP_MODEL_COMPLEXITY\n",
    ")\n",
    "\n",
    "geo_utils = GeometryUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a5dbe",
   "metadata": {},
   "source": [
    "## 5.3 Load ONNX Models and JSON Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf2cde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model for left loaded. Input name: 'float_input'\n",
      "JSON scaler parameters for left loaded.\n",
      "ONNX model for right loaded. Input name: 'float_input'\n",
      "JSON scaler parameters for right loaded.\n"
     ]
    }
   ],
   "source": [
    "def load_onnx_model_and_json_scaler(side, model_dir, model_type):\n",
    "    onnx_model_path = os.path.join(model_dir, f\"{model_type}_model_{side}.onnx\")\n",
    "    scaler_json_path = os.path.join(model_dir, f\"scaler_{side}.json\")\n",
    "    \n",
    "    ort_session, scaler_params, input_name = None, None, None\n",
    "\n",
    "    # Load ONNX model\n",
    "    if os.path.exists(onnx_model_path):\n",
    "        try:\n",
    "            ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "            input_name = ort_session.get_inputs()[0].name\n",
    "            print(f\"ONNX model for {side} loaded. Input name: '{input_name}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading ONNX model for {side} from {onnx_model_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"ONNX model file not found for {side}: {onnx_model_path}\")\n",
    "\n",
    "    # Load scaler parameters from JSON\n",
    "    if os.path.exists(scaler_json_path):\n",
    "        try:\n",
    "            with open(scaler_json_path, \"r\") as f_scaler_json:\n",
    "                scaler_data = json.load(f_scaler_json)\n",
    "            # Ensure mean and scale are numpy arrays and correctly shaped for broadcasting\n",
    "            scaler_params = {\n",
    "                'mean': np.array(scaler_data[\"mean\"], dtype=np.float32).reshape(1, -1),\n",
    "                'scale': np.array(scaler_data[\"scale\"], dtype=np.float32).reshape(1, -1)\n",
    "            }\n",
    "            # Verify scale is not zero\n",
    "            if np.any(scaler_params['scale'] == 0):\n",
    "                print(f\"Warning: Scaler for {side} has zero values in 'scale'. Replacing with 1.0 to avoid division by zero.\")\n",
    "                scaler_params['scale'][scaler_params['scale'] == 0] = 1.0\n",
    "            print(f\"JSON scaler parameters for {side} loaded.\")\n",
    "            # print(f\"  Scaler mean: {scaler_params['mean']}, Scaler scale: {scaler_params['scale']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading/parsing scaler JSON for {side} from {scaler_json_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Scaler JSON file not found for {side}: {scaler_json_path}\")\n",
    "        \n",
    "    return ort_session, scaler_params, input_name\n",
    "\n",
    "# Load for both arms\n",
    "ort_session_left, scaler_params_left, input_name_left = load_onnx_model_and_json_scaler(\"left\", ONNX_MODEL_DIR, MODEL_TYPE_LEFT)\n",
    "ort_session_right, scaler_params_right, input_name_right = load_onnx_model_and_json_scaler(\"right\", ONNX_MODEL_DIR, MODEL_TYPE_RIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040e633",
   "metadata": {},
   "source": [
    "## 5.4 Arm Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b55d0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmProcessor:\n",
    "    def __init__(self, side, ort_session, scaler_params, input_name, geo_utils):\n",
    "        self.side = side\n",
    "        self.ort_session = ort_session\n",
    "        self.scaler_mean = scaler_params['mean'] if scaler_params else None\n",
    "        self.scaler_scale = scaler_params['scale'] if scaler_params else None\n",
    "        self.input_name = input_name\n",
    "        self.geo_utils = geo_utils\n",
    "\n",
    "        self.is_ready = (self.ort_session is not None and\n",
    "                         self.scaler_mean is not None and\n",
    "                         self.scaler_scale is not None and\n",
    "                         self.input_name is not None)\n",
    "\n",
    "        self.state = \"DOWN\"  # Initial state: 'DOWN' or 'UP'\n",
    "        self.reps = 0\n",
    "        self.angle_history = deque(maxlen=SMOOTH_WINDOW_SIZE)\n",
    "        self.shoulder_neutral = None # Calibrated neutral shoulder angle\n",
    "        self.shoulder_angles_for_calibration = deque(maxlen=30) # Buffer for calibration\n",
    "        self.last_rep_time = 0\n",
    "        self.time_entered_up_state = 0\n",
    "        self.rep_quality_feedback = \"\" # e.g., \"Good rep!\", \"Too fast!\"\n",
    "        self.last_form_feedback_model = \"\" # Feedback from the ONNX model\n",
    "\n",
    "        if not self.is_ready:\n",
    "            print(f\"ArmProcessor for {self.side} side is NOT ready. ONNX Model, scaler params, or input name missing.\")\n",
    "\n",
    "    def calibrate_shoulder(self, current_shoulder_angle):\n",
    "        self.shoulder_angles_for_calibration.append(current_shoulder_angle)\n",
    "        # Calibrate once enough samples are collected and stable\n",
    "        if len(self.shoulder_angles_for_calibration) >= 15: # Require at least 15 samples\n",
    "            # Use median to be robust to outliers\n",
    "            self.shoulder_neutral = np.median(list(self.shoulder_angles_for_calibration))\n",
    "            # print(f\"[{self.side}] Shoulder calibrated to: {self.shoulder_neutral:.1f}°\")\n",
    "            self.shoulder_angles_for_calibration.clear() # Clear after calibration or if it drifts too much\n",
    "\n",
    "    def get_form_feedback_rules(self, elbow_angle_smoothed, shoulder_angle_smoothed):\n",
    "        feedback_messages = []\n",
    "        # Shoulder stability feedback\n",
    "        if self.shoulder_neutral is not None:\n",
    "            shoulder_deviation = shoulder_angle_smoothed - self.shoulder_neutral\n",
    "            if abs(shoulder_deviation) > SHOULDER_TOLERANCE:\n",
    "                feedback_messages.append(\"Stabilize shoulder\" if shoulder_deviation < 0 else \"Avoid leaning back\")\n",
    "        else:\n",
    "            feedback_messages.append(\"Calibrating shoulder...\")\n",
    "\n",
    "        # Elbow extension/squeeze feedback based on state\n",
    "        if self.state == \"DOWN\" and elbow_angle_smoothed < (ELBOW_EXTENSION_TARGET - 15):\n",
    "            feedback_messages.append(\"Extend arm fully\")\n",
    "        elif self.state == \"UP\" and elbow_angle_smoothed > (ELBOW_SQUEEZE_TARGET + 15):\n",
    "            feedback_messages.append(\"Squeeze bicep more\")\n",
    "\n",
    "        if not feedback_messages:\n",
    "            return \"Good form!\", COLOR_CORRECT_FORM\n",
    "        else:\n",
    "            return \" | \".join(feedback_messages), COLOR_FEEDBACK\n",
    "\n",
    "    def process_landmarks(self, landmarks_mp, frame_shape_wh):\n",
    "        frame_width, frame_height = frame_shape_wh\n",
    "        self.rep_quality_feedback = \"\" # Reset rep quality feedback each frame\n",
    "\n",
    "        if not self.is_ready:\n",
    "            return None, f\"{self.side.capitalize()} Model/Scaler Not Loaded\", COLOR_MODEL_ERROR, (0,0)\n",
    "\n",
    "        # Define MediaPipe landmarks for the current arm\n",
    "        lm_indices = {\n",
    "            \"shoulder\": mp_pose.PoseLandmark[f\"{self.side.upper()}_SHOULDER\"],\n",
    "            \"elbow\": mp_pose.PoseLandmark[f\"{self.side.upper()}_ELBOW\"],\n",
    "            \"wrist\": mp_pose.PoseLandmark[f\"{self.side.upper()}_WRIST\"],\n",
    "            \"hip\": mp_pose.PoseLandmark[f\"{self.side.upper()}_HIP\"]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Get landmark coordinates and visibility\n",
    "            actual_lms = {name: landmarks_mp[idx.value] for name, idx in lm_indices.items()}\n",
    "            \n",
    "            # Check visibility of all key landmarks\n",
    "            if not all(lm.visibility > MIN_VISIBILITY for lm in actual_lms.values()):\n",
    "                # print(f\"Low visibility for {self.side} arm: {[ (name, lm.visibility) for name, lm in actual_lms.items() if lm.visibility <= MIN_VISIBILITY]}\")\n",
    "                return None, f\"{self.side.capitalize()} arm not fully visible\", COLOR_VIS_LOW, (int(actual_lms['elbow'].x * frame_width) if actual_lms['elbow'].visibility > 0.1 else 0 , int(actual_lms['elbow'].y * frame_height) if actual_lms['elbow'].visibility > 0.1 else 0)\n",
    "\n",
    "            # Extract [x, y] coordinates\n",
    "            coords = {name: [lm.x, lm.y] for name, lm in actual_lms.items()}\n",
    "\n",
    "            # Calculate current angles\n",
    "            current_elbow_angle = self.geo_utils.calculate_angle(coords[\"shoulder\"], coords[\"elbow\"], coords[\"wrist\"])\n",
    "            current_shoulder_angle = self.geo_utils.calculate_angle(coords[\"elbow\"], coords[\"shoulder\"], coords[\"hip\"])\n",
    "            \n",
    "            if np.isnan(current_elbow_angle) or np.isnan(current_shoulder_angle):\n",
    "                return None, f\"Angle calc error {self.side}\", COLOR_INCORRECT_FORM, (int(coords['elbow'][0] * frame_width), int(coords['elbow'][1] * frame_height))\n",
    "\n",
    "            # Smooth angles\n",
    "            self.angle_history.append((current_elbow_angle, current_shoulder_angle))\n",
    "            if len(self.angle_history) < SMOOTH_WINDOW_SIZE:\n",
    "                elbow_angle_smoothed, shoulder_angle_smoothed = current_elbow_angle, current_shoulder_angle\n",
    "            else:\n",
    "                elbow_angle_smoothed, shoulder_angle_smoothed = np.mean(self.angle_history, axis=0)\n",
    "\n",
    "            # Shoulder calibration logic\n",
    "            # Calibrate only when arm is relatively straight (down phase) to get a neutral posture\n",
    "            if self.state == \"DOWN\" and elbow_angle_smoothed > SHOULDER_CALIBRATION_ANGLE_MIN and (self.shoulder_neutral is None or len(self.shoulder_angles_for_calibration)>0):\n",
    "                self.calibrate_shoulder(shoulder_angle_smoothed)\n",
    "            elif self.shoulder_neutral is not None and abs(shoulder_angle_smoothed - self.shoulder_neutral) > SHOULDER_TOLERANCE * 2: # If shoulder deviates too much, reset calibration buffer\n",
    "                self.shoulder_angles_for_calibration.clear() # Encourage re-calibration if posture changes significantly\n",
    "\n",
    "            # --- ONNX Model Prediction ---\n",
    "            features_unscaled = np.array([[elbow_angle_smoothed, shoulder_angle_smoothed]], dtype=np.float32)\n",
    "            features_scaled = (features_unscaled - self.scaler_mean) / self.scaler_scale\n",
    "            \n",
    "            input_feed = {self.input_name: features_scaled}\n",
    "            pred_onnx = self.ort_session.run(None, input_feed)\n",
    "            # Assuming model output is [prediction_class_index], and 1=Correct, 0=Incorrect\n",
    "            model_form_prediction = pred_onnx[0][0] # This is the predicted class (0 or 1)\n",
    "            \n",
    "            is_form_correct_model = (model_form_prediction == 1) # 1 for 'C' (Correct)\n",
    "            self.last_form_feedback_model = \"Form OK (Model)\" if is_form_correct_model else \"Fix Form (Model)\"\n",
    "            model_feedback_color = COLOR_CORRECT_FORM if is_form_correct_model else COLOR_INCORRECT_FORM\n",
    "\n",
    "            # --- Rep Counting & Rule-Based Feedback ---\n",
    "            current_time = time.time()\n",
    "            rule_based_feedback_text, rule_based_feedback_color = self.get_form_feedback_rules(elbow_angle_smoothed, shoulder_angle_smoothed)\n",
    "            \n",
    "            # Combine model feedback and rule-based feedback\n",
    "            # Priority: If model says form is incorrect, that takes precedence for display color.\n",
    "            final_feedback_text = f\"{self.last_form_feedback_model} | {rule_based_feedback_text}\"\n",
    "            final_feedback_color = model_feedback_color if not is_form_correct_model else rule_based_feedback_color\n",
    "\n",
    "            # Rep counting logic (only if model predicts good form or if we allow reps with minor rule infractions)\n",
    "            # For simplicity, let's allow rep counting even if rule-based feedback is active, but model must say OK.\n",
    "            if is_form_correct_model and (current_time - self.last_rep_time > REP_COOLDOWN_SECONDS):\n",
    "                if self.state == \"DOWN\" and elbow_angle_smoothed < ELBOW_ANGLE_UP_THRESHOLD:\n",
    "                    self.state = \"UP\"\n",
    "                    self.time_entered_up_state = current_time\n",
    "                elif self.state == \"UP\" and elbow_angle_smoothed > ELBOW_ANGLE_DOWN_THRESHOLD:\n",
    "                    self.state = \"DOWN\"\n",
    "                    self.last_rep_time = current_time\n",
    "                    if self.time_entered_up_state != 0:\n",
    "                        rep_duration = current_time - self.time_entered_up_state\n",
    "                        if MIN_REP_DURATION_SECONDS <= rep_duration <= MAX_REP_DURATION_SECONDS:\n",
    "                            self.reps += 1\n",
    "                            self.rep_quality_feedback = \"Good rep!\"\n",
    "                        elif rep_duration < MIN_REP_DURATION_SECONDS:\n",
    "                            self.rep_quality_feedback = \"Rep too fast!\"\n",
    "                        else:\n",
    "                            self.rep_quality_feedback = \"Rep too slow!\"\n",
    "                    else: # Should not happen if logic is correct\n",
    "                        self.rep_quality_feedback = \"Timing error\"\n",
    "                    self.time_entered_up_state = 0 # Reset for next rep\n",
    "            elif not is_form_correct_model and self.state == \"UP\":\n",
    "                # If form breaks mid-rep (UP state), reset state to DOWN to prevent counting bad rep completion\n",
    "                self.state = \"DOWN\"\n",
    "                self.time_entered_up_state = 0\n",
    "                self.rep_quality_feedback = \"Form broke mid-rep!\"\n",
    "\n",
    "            # Position for angle text (near elbow)\n",
    "            angle_text_coords = (int(coords['elbow'][0] * frame_width) + 10, int(coords['elbow'][1] * frame_height))\n",
    "            \n",
    "            return int(elbow_angle_smoothed), final_feedback_text, final_feedback_color, angle_text_coords\n",
    "\n",
    "        except IndexError:\n",
    "            # print(f\"IndexError: Landmark not found for {self.side} arm.\") # Expected if person not fully in frame\n",
    "            return None, f\"{self.side.capitalize()} landmarks missing\", COLOR_VIS_LOW, (0,0)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"Error processing {self.side} arm: {e}\")\n",
    "            # traceback.print_exc() # Uncomment for detailed stack trace\n",
    "            return None, f\"Error {self.side}\", COLOR_INCORRECT_FORM, (0,0)\n",
    "\n",
    "    def reset_reps(self):\n",
    "        self.reps = 0\n",
    "        self.state = \"DOWN\"\n",
    "        self.angle_history.clear()\n",
    "        self.shoulder_angles_for_calibration.clear()\n",
    "        self.shoulder_neutral = None\n",
    "        self.last_rep_time = 0\n",
    "        self.time_entered_up_state = 0\n",
    "        self.rep_quality_feedback = \"\"\n",
    "        print(f\"[{self.side.upper()}] Reps, state, and calibration reset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8f738",
   "metadata": {},
   "source": [
    "## 5.5 Initialize Arm Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5eb49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_processor = ArmProcessor(\"left\", ort_session_left, scaler_params_left, input_name_left, geo_utils)\n",
    "right_processor = ArmProcessor(\"right\", ort_session_right, scaler_params_right, input_name_right, geo_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8408292",
   "metadata": {},
   "source": [
    "## 5.6 Main Video Processing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_camera_loop = False\n",
    "\n",
    "def video_processing_loop():\n",
    "    global stop_camera_loop\n",
    "    stop_camera_loop = False\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video source.\")\n",
    "        return\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    print(\"Starting Bicep Curl Trainer. Press 'q' in the OpenCV window to quit, 'r' to reset reps.\")\n",
    "    print(f\"Target rep duration: {MIN_REP_DURATION_SECONDS:.1f}s - {MAX_REP_DURATION_SECONDS:.1f}s\")\n",
    "    \n",
    "    cv_window_name = 'Bicep Curl AI Trainer (ONNX)'\n",
    "    cv2.namedWindow(cv_window_name, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    while cap.isOpened() and not stop_camera_loop:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame or end of video.\")\n",
    "            if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT) and cap.get(cv2.CAP_PROP_FRAME_COUNT) > 0:\n",
    "                 break\n",
    "            continue\n",
    "\n",
    "        frame_to_process = frame \n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame_to_process, cv2.COLOR_BGR2RGB)\n",
    "        image_rgb.flags.writeable = False \n",
    "        results = pose_detector.process(image_rgb)\n",
    "        image_rgb.flags.writeable = True \n",
    "        output_image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                output_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=1)\n",
    "            )\n",
    "\n",
    "        display_frame = cv2.flip(output_image, 1)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            processors = [left_processor, right_processor]\n",
    "            for i, processor in enumerate(processors):\n",
    "                feedback_base_y = 70\n",
    "                text_x_pos_orig = 10 if processor.side == \"left\" else FRAME_WIDTH - 450 \n",
    "                \n",
    "                angle_val, form_fb_str, form_fb_col, angle_text_coords_orig = processor.process_landmarks(\n",
    "                    results.pose_landmarks.landmark,\n",
    "                    (FRAME_WIDTH, FRAME_HEIGHT)\n",
    "                )\n",
    "                \n",
    "                current_y_offset = feedback_base_y + (i * 100) \n",
    "                if processor.side == \"right\": current_y_offset = feedback_base_y\n",
    "\n",
    "                if angle_val is not None and angle_text_coords_orig != (0,0):\n",
    "                    text_angle = f\"{angle_val}°\"\n",
    "                    (text_w, _), _ = cv2.getTextSize(text_angle, FONT, 0.7, 2)\n",
    "                    orig_x_angle, orig_y_angle = angle_text_coords_orig\n",
    "                    new_x_angle = FRAME_WIDTH - orig_x_angle - text_w\n",
    "                    cv2.putText(display_frame, text_angle, (new_x_angle, orig_y_angle),\n",
    "                                FONT, 0.7, form_fb_col, 2, cv2.LINE_AA)\n",
    "\n",
    "                if form_fb_str:\n",
    "                    words = form_fb_str.split(' ')\n",
    "                    lines = []\n",
    "                    current_line = \"\"\n",
    "                    max_line_width_px = 420 \n",
    "                    for word_idx, word in enumerate(words):\n",
    "                        test_line = f\"{current_line} {word}\".strip()\n",
    "                        (line_w, _), _ = cv2.getTextSize(test_line, FONT, 0.6, 1)\n",
    "                        if line_w > max_line_width_px and current_line:\n",
    "                            lines.append(current_line)\n",
    "                            current_line = word\n",
    "                        else:\n",
    "                            current_line = test_line\n",
    "                        if word_idx == len(words) - 1:\n",
    "                            lines.append(current_line)\n",
    "                    \n",
    "                    for line_idx, line_text in enumerate(lines):\n",
    "                        (text_w, _), _ = cv2.getTextSize(line_text, FONT, 0.6, 1)\n",
    "                        new_x_line = FRAME_WIDTH - text_x_pos_orig - text_w\n",
    "                        cv2.putText(display_frame, line_text, (new_x_line, current_y_offset + line_idx * 20),\n",
    "                                   FONT, 0.6, form_fb_col, 2, cv2.LINE_AA)\n",
    "                    current_y_offset += len(lines) * 20 + 5\n",
    "\n",
    "                if processor.rep_quality_feedback:\n",
    "                    rep_fb_color = COLOR_CORRECT_FORM if \"Good\" in processor.rep_quality_feedback else COLOR_INCORRECT_FORM\n",
    "                    text_rep_quality = processor.rep_quality_feedback\n",
    "                    (text_w, _), _ = cv2.getTextSize(text_rep_quality, FONT, 0.7, 2)\n",
    "                    new_x_rep_quality = FRAME_WIDTH - text_x_pos_orig - text_w\n",
    "                    cv2.putText(display_frame, text_rep_quality,\n",
    "                                (new_x_rep_quality, current_y_offset),\n",
    "                                FONT, 0.7, rep_fb_color, 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            text_no_person = \"No person detected\"\n",
    "            (text_w, _), _ = cv2.getTextSize(text_no_person, FONT, 1, 2)\n",
    "            orig_x_no_person = FRAME_WIDTH // 2 - 150\n",
    "            new_x_no_person = FRAME_WIDTH - orig_x_no_person - text_w\n",
    "            cv2.putText(display_frame, text_no_person, (new_x_no_person, FRAME_HEIGHT // 2),\n",
    "                        FONT, 1, COLOR_INCORRECT_FORM, 2, cv2.LINE_AA)\n",
    "\n",
    "        text_left_reps = f\"Left Reps: {left_processor.reps}\"\n",
    "        (text_w_left, _), _ = cv2.getTextSize(text_left_reps, FONT, 1, 2)\n",
    "        orig_x_left_reps = 10\n",
    "        new_x_left_reps = FRAME_WIDTH - orig_x_left_reps - text_w_left\n",
    "        cv2.putText(display_frame, text_left_reps, (new_x_left_reps, 40),\n",
    "                    FONT, 1, COLOR_NEUTRAL, 2, cv2.LINE_AA)\n",
    "        \n",
    "        text_right_reps = f\"Right Reps: {right_processor.reps}\"\n",
    "        (text_w_right, _), _ = cv2.getTextSize(text_right_reps, FONT, 1, 2)\n",
    "        orig_x_right_reps = FRAME_WIDTH - 280\n",
    "        new_x_right_reps = FRAME_WIDTH - orig_x_right_reps - text_w_right\n",
    "        cv2.putText(display_frame, text_right_reps, (new_x_right_reps, 40),\n",
    "                    FONT, 1, COLOR_NEUTRAL, 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow(cv_window_name, display_frame)\n",
    "\n",
    "        key = cv2.waitKey(5) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print(\"Quit key pressed. Stopping detection.\")\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            print(\"Resetting reps for both arms.\")\n",
    "            left_processor.reset_reps()\n",
    "            right_processor.reset_reps()\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    for i in range(5):\n",
    "        cv2.waitKey(1)\n",
    "    print(\"Video processing loop finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f1c0a",
   "metadata": {},
   "source": [
    "### How to Run the Live Detection:\n",
    "\n",
    "1.  Make sure you have executed all previous cells to load models, scalers, and define classes/functions.\n",
    "2.  Uncomment the line `video_processing_loop()` in the cell above (or the threading lines if you prefer non-blocking behavior, though direct call is simpler for typical notebook use).\n",
    "3.  Run the cell. An OpenCV window should appear showing your webcam feed with detections.\n",
    "4.  **Press 'q' in the OpenCV window** to close it and stop the detection loop.\n",
    "5.  **Press 'r' in the OpenCV window** to reset rep counts and arm states.\n",
    "\n",
    "If the OpenCV window doesn't close properly or the kernel seems stuck after pressing 'q', you might need to:\n",
    "- Interrupt the kernel (from the Kernel menu in Jupyter).\n",
    "- Restart the kernel.\n",
    "\n",
    "**Stopping the loop from another cell (if using threading and `stop_camera_loop` global):**\n",
    "```python\n",
    "# global stop_camera_loop\n",
    "# stop_camera_loop = True\n",
    "# if 'video_thread' in globals() and video_thread.is_alive():\n",
    "#     video_thread.join(timeout=2) # Wait for thread to finish\n",
    "#     print(\"Video thread stopped.\")\n",
    "# cv2.destroyAllWindows()\n",
    "```\n",
    "For simplicity in a notebook, directly calling `video_processing_loop()` is often easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfa8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and scalers seem to be loaded. Ready to start video processing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bicep Curl Trainer. Press 'q' in the OpenCV window to quit, 'r' to reset reps.\n",
      "Target rep duration: 0.4s - 3.0s\n",
      "Ignoring empty camera frame or end of video.\n",
      "Video processing loop finished.\n",
      "Ignoring empty camera frame or end of video.\n",
      "Video processing loop finished.\n"
     ]
    }
   ],
   "source": [
    "if ort_session_left and ort_session_right and scaler_params_left and scaler_params_right:\n",
    "    print(\"Models and scalers seem to be loaded. Ready to start video processing.\")\n",
    "    video_processing_loop()\n",
    "else:\n",
    "    print(\"ONNX models or scalers are not loaded properly. Cannot start live detection.\")\n",
    "    print(f\"  Left Session: {'OK' if ort_session_left else 'FAIL'}, Left Scaler: {'OK' if scaler_params_left else 'FAIL'}\")\n",
    "    print(f\"  Right Session: {'OK' if ort_session_right else 'FAIL'}, Right Scaler: {'OK' if scaler_params_right else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6279a",
   "metadata": {},
   "source": [
    "After running the live detection, if the OpenCV window doesn't close cleanly or you need to force stop, you might need to interrupt or restart the Jupyter kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
